{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sankaranarayanan <br />\n",
    " 06/21/2017 <br />\n",
    "\n",
    "### Approach <br />\n",
    "The given market data is extracted , class imbalance is negated and finally data is feature engineered. Keras open source neural network is used to build and artificial neural network on top of the engineered data. The hidded layers are altered, specific solvers are used keeping in mind the binary outcome. Rmsprop has been known to work with binary outcomes and hence it is used to build the ANN. Suitable loss functions and metrics are introduced to measure the goodness of the model. Optimal epoch size is selected to narrow down on the suitable ROC results. Precision Recall ROC and  Area Under the Receiver Operating Characteristic curve are estimated and suitable plots are created to analyze optimal AUROC. <br />\n",
    "\n",
    "Estimated AUC on test.csv = 0.850748669861 <br />\n",
    "\n",
    "\n",
    "### Challenge in creation \n",
    "Optimality of solvers, epoch size and hidden layers are critical in constructing this model. The feature selection is also challenging and RandomizedLogisticRegression proved to be more effective amongst other selection methods. The creation of Random forest ensemble models , decision trees with bagging and SVC were also tried, but teh Keras ANN was the one which exhibited maximum improvement in results at each model improvement stage<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score, accuracy_score, average_precision_score, precision_recall_curve, hamming_loss\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn import preprocessing\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "#intitalize neural network\n",
    "from keras.models import Sequential\n",
    "#create ANN layers\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import RandomizedLogisticRegression\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def imputevals(dataset):  \n",
    "    \"\"\"\n",
    "    Operations:\n",
    "    1. Column wise check for missing values\n",
    "    2. Impute with most frequent if columns are of type Object\n",
    "    3. Impute with mean if columns are not of type Object\n",
    "    4. return dataset \n",
    "    \"\"\"\n",
    "    \n",
    "    k= pd.Series([dataset[c].value_counts().index[0]\n",
    "        if dataset[c].dtype == np.dtype('O') else dataset[c].mean() for c in dataset],\n",
    "        index=dataset.columns)\n",
    "    return dataset.fillna(k)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def catencode(x):\n",
    "    \"\"\"\n",
    "    Operations:\n",
    "    1. Encode categorical variables\n",
    "    2. Escape the dummy variable trap\n",
    "    3. return encoded X  \n",
    "    \"\"\"\n",
    "   \n",
    "    \n",
    "\n",
    "    lblencoder_x = LabelEncoder()\n",
    "\n",
    "    #dummy variable encoder\n",
    "    #onehotencoder_x = OneHotEncoder(categorical_features = [0])\n",
    "\n",
    "    x[:,2]=lblencoder_x.fit_transform(x[:,2])\n",
    "    #onehotencoder = OneHotEncoder(categorical_features = [2])\n",
    "    \n",
    "    x[:,3]=lblencoder_x.fit_transform(x[:,3])\n",
    "    #onehotencoder = OneHotEncoder(categorical_features = [3])\n",
    "    #Xf = onehotencoder.fit_transform(X).toarray()\n",
    "    onehotencoder = OneHotEncoder(categorical_features = [2,3])\n",
    "    X = onehotencoder.fit_transform(x).toarray()\n",
    "    X= np.delete(X,0,1)\n",
    "    X= np.delete(X,4,1)\n",
    "\n",
    "    \n",
    "    return X\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scalerfunc(x):\n",
    "    \"\"\"\n",
    "    Scales all indepedant variables so that computational\n",
    "    efficiency is improved for our model\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    x_scal = scaler.fit_transform(x)\n",
    "    \n",
    "    return x_scal\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ANN(classifier,X_train,y_train,X_test,y_test,n,Xtestdata):\n",
    "    \n",
    "    \"\"\"\n",
    "    ANN model\n",
    "    Parameters used:\n",
    "    out_dim - number of nodes in input layer\n",
    "    init - Initialize weights to small numbers close to 0\n",
    "    (Glorot with weights sampled from the normal distribution)\n",
    "    activation - rectifier function , relu \n",
    "    input_dim - number of input or indepedant features\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 8))\n",
    "    \n",
    "    # Adding the hidden layers\n",
    "    classifier.add(Dense(output_dim = 6, init = 'glorot_normal', activation = 'relu'))\n",
    "    classifier.add(Dense(output_dim = 6, init = 'glorot_normal', activation = 'relu'))\n",
    "    classifier.add(Dense(output_dim = 5, init = 'glorot_normal', activation = 'relu'))\n",
    "    classifier.add(Dense(output_dim = 5, init = 'glorot_normal', activation = 'relu'))\n",
    "    classifier.add(Dense(output_dim = 5, init = 'glorot_normal', activation = 'relu'))\n",
    "        \n",
    "    # Adding the output layer\n",
    "    classifier.add(Dense(output_dim = 1, init = 'glorot_normal', activation = 'sigmoid'))\n",
    "        \n",
    "    # Compiling the ANN\n",
    "    classifier.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)\n",
    "\n",
    "    # Part 3 - Making the predictions and evaluating the model\n",
    "\n",
    "    # Predicting the Test set results\n",
    "    #pred = classifier.predict_proba(X_test)\n",
    "    y_pred = classifier.predict_proba(X_test)\n",
    "    y_testpred = classifier.predict_proba(Xtestdata)\n",
    "    \n",
    "    \"\"\"\n",
    "    y_pred=[]\n",
    "    for x in pred:\n",
    "        if x>0.5:\n",
    "            pred.append[1]\n",
    "        else:\n",
    "            pred.append[0]\n",
    "    \"\"\"\n",
    "    \n",
    "    y_predc = classifier.predict_proba(X_test)\n",
    "    \n",
    "    # Compute Precision-Recall and plot curve\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    average_precision = dict()\n",
    "    for i in range(n):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_test[:,i],\n",
    "                                                            y_pred[:,i])\n",
    "        average_precision[i] = average_precision_score(y_test[:,i], y_pred[:,i])\n",
    "    \n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test.ravel(),\n",
    "        y_pred.ravel())\n",
    "    average_precision[\"micro\"] = average_precision_score(y_test, y_pred,\n",
    "                                                         average=\"micro\")\n",
    "    # Visualizing the results\n",
    "    \n",
    "    \n",
    "    # Plot Precision-Recall curve\n",
    "    plt.clf()\n",
    "    plt.plot(recall[0], precision[0], lw=2, color='navy',\n",
    "             label='Precision-Recall curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Precision-Recall example: PR AUC={0:0.3f}'.format(average_precision[0]))\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_pred.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"],tpr[\"micro\"])\n",
    "        \n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr[0], tpr[0], color='navy',\n",
    "             lw=lw, label='ROC curve (area = %0.3f)' % roc_auc[0])\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()    \n",
    "        \n",
    "    \n",
    "    aucval = metrics.roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return aucval,y_testpred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readdata(directory,filename):\n",
    "    os.chdir(directory)\n",
    "    dataset = pd.read_csv(filename)\n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imbalanced(dataset):\n",
    "    zero = dataset[trainset['outcome'] == 0]\n",
    "    one = dataset[trainset['outcome'] == 1]\n",
    "    # get the data of each class\n",
    "    if len(one) > len(zero):\n",
    "        index = one.index[len(zero):]\n",
    "    else:\n",
    "        index = zero.index[len(one):]\n",
    "\n",
    "    dataset.drop(index, inplace = True)\n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of main\n",
      "Import data\n",
      "Missing data has been imputed\n",
      "Solve class Imbalance in trainingset\n",
      "Encoding\n",
      "Categorical data has been encoded\n",
      "Scaling data\n",
      "Feature Selection\n",
      "Train Test Split\n",
      "Epoch 1/100\n",
      "1374/1374 [==============================] - 9s - loss: 0.6833 - acc: 0.5837     \n",
      "Epoch 2/100\n",
      "1374/1374 [==============================] - 1s - loss: 0.6268 - acc: 0.7111     \n",
      "Epoch 3/100\n",
      "1374/1374 [==============================] - 1s - loss: 0.5822 - acc: 0.7314     \n",
      "Epoch 4/100\n",
      "1374/1374 [==============================] - 1s - loss: 0.5602 - acc: 0.7278     \n",
      "Epoch 5/100\n",
      "1374/1374 [==============================] - 1s - loss: 0.5472 - acc: 0.7358     \n",
      "Epoch 6/100\n",
      "1374/1374 [==============================] - 1s - loss: 0.5408 - acc: 0.7329     \n",
      "Epoch 7/100\n",
      "1374/1374 [==============================] - 1s - loss: 0.5363 - acc: 0.7409     \n",
      "Epoch 8/100\n",
      "1374/1374 [==============================] - 1s - loss: 0.5323 - acc: 0.7482     \n",
      "Epoch 9/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5301 - acc: 0.7445     \n",
      "Epoch 10/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5276 - acc: 0.7518     \n",
      "Epoch 11/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5268 - acc: 0.7525     \n",
      "Epoch 12/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5256 - acc: 0.7460     \n",
      "Epoch 13/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5241 - acc: 0.7525     \n",
      "Epoch 14/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5234 - acc: 0.7547     \n",
      "Epoch 15/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5232 - acc: 0.7475     \n",
      "Epoch 16/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5210 - acc: 0.7555     \n",
      "Epoch 17/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5202 - acc: 0.7504     \n",
      "Epoch 18/100\n",
      "1374/1374 [==============================] - 1s - loss: 0.5200 - acc: 0.7504     \n",
      "Epoch 19/100\n",
      "1374/1374 [==============================] - 1s - loss: 0.5185 - acc: 0.7489     \n",
      "Epoch 20/100\n",
      "1374/1374 [==============================] - 1s - loss: 0.5163 - acc: 0.7569     \n",
      "Epoch 21/100\n",
      "1374/1374 [==============================] - 1s - loss: 0.5159 - acc: 0.7562     \n",
      "Epoch 22/100\n",
      "1374/1374 [==============================] - 1s - loss: 0.5140 - acc: 0.7606     \n",
      "Epoch 23/100\n",
      "1374/1374 [==============================] - 1s - loss: 0.5130 - acc: 0.7598     \n",
      "Epoch 24/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5132 - acc: 0.7576     \n",
      "Epoch 25/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5121 - acc: 0.7649     \n",
      "Epoch 26/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5098 - acc: 0.7591     \n",
      "Epoch 27/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5115 - acc: 0.7598     \n",
      "Epoch 28/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5097 - acc: 0.7584     \n",
      "Epoch 29/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5087 - acc: 0.7635     \n",
      "Epoch 30/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5083 - acc: 0.7627     \n",
      "Epoch 31/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5071 - acc: 0.7576     \n",
      "Epoch 32/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5071 - acc: 0.7627     \n",
      "Epoch 33/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5063 - acc: 0.7649     \n",
      "Epoch 34/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5053 - acc: 0.7700     \n",
      "Epoch 35/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5058 - acc: 0.7591     \n",
      "Epoch 36/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5047 - acc: 0.7627     \n",
      "Epoch 37/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5048 - acc: 0.7598     \n",
      "Epoch 38/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5033 - acc: 0.7591     \n",
      "Epoch 39/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5039 - acc: 0.7671     \n",
      "Epoch 40/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5031 - acc: 0.7664     \n",
      "Epoch 41/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5012 - acc: 0.7656     \n",
      "Epoch 42/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5032 - acc: 0.7613     \n",
      "Epoch 43/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5014 - acc: 0.7671     \n",
      "Epoch 44/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5009 - acc: 0.7693     \n",
      "Epoch 45/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5019 - acc: 0.7606     \n",
      "Epoch 46/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5009 - acc: 0.7627     \n",
      "Epoch 47/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4977 - acc: 0.7693     \n",
      "Epoch 48/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5019 - acc: 0.7671     \n",
      "Epoch 49/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5005 - acc: 0.7737     \n",
      "Epoch 50/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.5004 - acc: 0.7693     \n",
      "Epoch 51/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4996 - acc: 0.7686     \n",
      "Epoch 52/100\n",
      "1374/1374 [==============================] - 1s - loss: 0.5011 - acc: 0.7693     \n",
      "Epoch 53/100\n",
      "1374/1374 [==============================] - 1s - loss: 0.4991 - acc: 0.7649     \n",
      "Epoch 54/100\n",
      "1374/1374 [==============================] - 1s - loss: 0.4990 - acc: 0.7729     \n",
      "Epoch 55/100\n",
      "1374/1374 [==============================] - 1s - loss: 0.4979 - acc: 0.7758     \n",
      "Epoch 56/100\n",
      "1374/1374 [==============================] - 1s - loss: 0.4978 - acc: 0.7678     \n",
      "Epoch 57/100\n",
      "1374/1374 [==============================] - 1s - loss: 0.4995 - acc: 0.7627     \n",
      "Epoch 58/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4968 - acc: 0.7744     \n",
      "Epoch 59/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4982 - acc: 0.7656     \n",
      "Epoch 60/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4985 - acc: 0.7649     \n",
      "Epoch 61/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4962 - acc: 0.7678     \n",
      "Epoch 62/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4982 - acc: 0.7678     \n",
      "Epoch 63/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4973 - acc: 0.7664     \n",
      "Epoch 64/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4951 - acc: 0.7656     \n",
      "Epoch 65/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4968 - acc: 0.7744     \n",
      "Epoch 66/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4962 - acc: 0.7744     \n",
      "Epoch 67/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4971 - acc: 0.7715     \n",
      "Epoch 68/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4953 - acc: 0.7686     \n",
      "Epoch 69/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4976 - acc: 0.7649     \n",
      "Epoch 70/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4977 - acc: 0.7700     \n",
      "Epoch 71/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4960 - acc: 0.7642     \n",
      "Epoch 72/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4947 - acc: 0.7715     \n",
      "Epoch 73/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4952 - acc: 0.7693     \n",
      "Epoch 74/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4961 - acc: 0.7744     \n",
      "Epoch 75/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4965 - acc: 0.7649     \n",
      "Epoch 76/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4965 - acc: 0.7686     \n",
      "Epoch 77/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4961 - acc: 0.7671     \n",
      "Epoch 78/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4964 - acc: 0.7693     \n",
      "Epoch 79/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4944 - acc: 0.7751     \n",
      "Epoch 80/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4953 - acc: 0.7686     \n",
      "Epoch 81/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4952 - acc: 0.7686     \n",
      "Epoch 82/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4958 - acc: 0.7700     \n",
      "Epoch 83/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4939 - acc: 0.7737     \n",
      "Epoch 84/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4936 - acc: 0.7737     \n",
      "Epoch 85/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4938 - acc: 0.7686     \n",
      "Epoch 86/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4943 - acc: 0.7722     \n",
      "Epoch 87/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4947 - acc: 0.7737     \n",
      "Epoch 88/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4941 - acc: 0.7664     \n",
      "Epoch 89/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4938 - acc: 0.7700     \n",
      "Epoch 90/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4926 - acc: 0.7671     \n",
      "Epoch 91/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4934 - acc: 0.7642     \n",
      "Epoch 92/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4925 - acc: 0.7787     \n",
      "Epoch 93/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4924 - acc: 0.7686     \n",
      "Epoch 94/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4942 - acc: 0.7700     \n",
      "Epoch 95/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4926 - acc: 0.7715     \n",
      "Epoch 96/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4945 - acc: 0.7744     \n",
      "Epoch 97/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4927 - acc: 0.7722     \n",
      "Epoch 98/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4928 - acc: 0.7678     \n",
      "Epoch 99/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4893 - acc: 0.7780     \n",
      "Epoch 100/100\n",
      "1374/1374 [==============================] - 0s - loss: 0.4921 - acc: 0.7693     \n",
      "352/590 [================>.............] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "if __name__  == \"__main__\":\n",
    "    print(\"Start of main\")\n",
    "    print(\"Import data\")\n",
    "    directory= os.getcwd()\n",
    "    filename = 'train.csv'\n",
    "    trainset = readdata(directory,filename)\n",
    "    filename = 'test.csv'\n",
    "    testset = readdata(directory,filename)\n",
    "    \n",
    "    trainset = imputevals(trainset)\n",
    "    testset = imputevals(testset)\n",
    "    print(\"Missing data has been imputed\")\n",
    "\n",
    "    print(\"Solve class Imbalance in trainingset\")\n",
    "    trainset = imbalanced(trainset)\n",
    "    \n",
    "    x= trainset.iloc[:,:-1].values\n",
    "\n",
    "    y=trainset.iloc[:,9].values\n",
    "\n",
    "\n",
    "    print(\"Encoding\")\n",
    "\n",
    "    X= catencode(x)\n",
    "    xtestdata= testset.iloc[:,:].values\n",
    "\n",
    "    Xtestdata= catencode(xtestdata)\n",
    "    \n",
    "    print(\"Categorical data has been encoded\")\n",
    "    \n",
    "    print(\"Scaling data\")\n",
    "    X_scal= scalerfunc(X)\n",
    "    Xtestdata= scalerfunc(Xtestdata)\n",
    "    \n",
    "    print(\"Feature Selection\")\n",
    "\n",
    "    rlr = RandomizedLogisticRegression()\n",
    "    x_train = rlr.fit_transform(X_scal,y)\n",
    "    pca = PCA(n_components=8)\n",
    "    Xtestdata=pca.fit_transform(Xtestdata)\n",
    "    print(\"Train Test Split\")\n",
    "    y = label_binarize(y, classes=[0, 1])\n",
    "    n = y.shape[1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_train, y, test_size = 0.3, random_state = 1)\n",
    "    \n",
    "    classifier = Sequential()\n",
    "    ROauc,testprediction=ANN(classifier,X_train,y_train,X_test,y_test,n,Xtestdata)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.850748669861\n"
     ]
    }
   ],
   "source": [
    "print(ROauc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results where got when the randomized logistic regression method of feture selection selected 8 features for modelling, it is also \n",
    "possible that the sleection method may select 7 features. In that case the selection process is re run as we have the maximum\n",
    "results with the 8 most prominent features selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probability values of positive outcome for test.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.31915501],\n",
       "       [ 0.44646215],\n",
       "       [ 0.93447131],\n",
       "       ..., \n",
       "       [ 0.34713653],\n",
       "       [ 0.52658176],\n",
       "       [ 0.72054708]], dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Predicted probability values of positive outcome for test.csv\" )\n",
    "testprediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting a hard probability limit of 0.5 to segregate between the two classes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print( \"Setting a hard probability limit of 0.5 to segregate between the two classes\")\n",
    "\n",
    "test_pred=[]\n",
    "for x in testprediction:\n",
    "    if x>0.5:\n",
    "            test_pred.append(1)\n",
    "    else:\n",
    "            test_pred.append(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The outcome for test.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"The outcome for test.csv\")\n",
    "\n",
    "\n",
    "test_pred"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
